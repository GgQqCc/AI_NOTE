VAE：变分自编码器。输入编码成潜在空间的概率分布，学习平滑连续的潜在表示。
传统自编码器给出潜在向量z  vae给出 u 和segema方，由这个概率分布随机采样一个z给解码器。
重参数化剥离随机性：z = u + segama* qieta   qieta是0-1之间随机变量。
潜在向量z确定性向量，可训练
损失：重构损失（恢复原样），KL散度损失。
视觉VAE
Ft VAE 运动特征
位置姿态speed
action（动作）拼接给ddpg作为输入。
神经网路架构：MLP
DDPGfd：深度确定性策略梯度算法  DQN目标网络和经验回放机制。
DDPG+ hybrid replay buffer  演示数据加入缓冲区，采样时追踪demo_ratio
理论框架是MDP：序列决策问题，即在给定的状态S、动作A、转移概率 P 和回报 r下，找到一个最优策略pi，以最大化长期累计回报。
训练初期快速成功，解决探索效率低问题
损失函数：更新策略网络（根据s和a，拿到评估与目标网络的误差mse）+Q函数网络（actor，最小化负损失函数，给动作打分）
目标网络：polyak 0.005 软更新（不会梯度降那么快，每次只更新一点）的 actor 和ctric。

导纳：
M D K
力位混合控制


replay buffer s_t：t 时刻的状态（State）a_t：t 时刻采取的动作（Action）r_t：t 时刻获得的奖励（Reward）s_{t+1}：t+1 时刻的新状态d：表示是否结束（Done）
经验回放机制 （随机采样（训练时随机拿存储的经验训），存储经验）
目的：打破时间相关性，iid 独立同分布假设；经验重复使用，减少了交互次数。
