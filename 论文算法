pi 0.6，RECAP 把feedback融入vla训练流程。
基于优势条件策略的经验和修正强化学习
它是在真机rollout p阶段
人类专家实时观察 Rollout 过程，并在模型犯错时提供干预/修正。
：人类修正可以被赋予非常高的A值（例如 $A=5.0$），确保这些高质量、少量的修正数据对策略的学习产生极强的影响，迅速纠正模型行为
用了ACPs优势条件策略

1 离线rl预训练，具备广泛通用技能
2 模仿学习微调模型，设定良好初始策略 finetuning
3、在位数据收集，RL改进。
执行个迭代的在线RL (或利用奖励反馈的离线 RL) 循环。
利用自主执行中收集到的经验，结合人类奖励反馈和干预来进一步提高策略性能。

类似PPO风格优势估计GAE和人类奖励偏好融合 类似rlhf。
数据是动态收集的（机器人执行任务，产生新的经验），并结合了稀疏的任务成功奖励和人类干预（例如，人类纠正错误时提供的奖励信号），实现高效的策略提升。

基于流/ 扩散模型的VLA
VLA 模型名称,核心机制,特点
Diffusion Policy,使用 扩散模型 (Diffusion Model) 来建模动作序列的条件分布。
能够生成高质量、多模式的动作，在处理多任务和不确定性高的环境时表现优异。
πRL​ (如上文),基于流的 VLA，将 RL 融入训练，提升在线适应性。
专注于解决 VLA 模型的在线微调和动作对数似然计算问题。

强化学习：
1. 基于值函数(Value-Based) ：主要学习一个值函数，用来估计在特定状态或状态-动作对下能获得的预期累积奖励
Q-Learning,"学习最优的 Q-函数 Q(s,a)。" DQN
适用于离散状态和动作空间，是 Model-Free RL 的基础。
2、基于策略（Policy-Based）：核心是直接学习一个策略函数pi(a|s)，并尝试最大化其带来的期望回报。
算法, REINFORCE：原理：使用策略梯度直接更新策略参数 threata。它通过一个完整回合（Episode）的蒙特卡洛回报Gt来估计动作价值，并沿梯度方向优化。
3、AC算法：PPO 在Actor更新中引入裁剪惩罚，确保新旧策略差距小。



原理,使用策略梯度定理直接更新策略参数 θ。它通过一个完整回合（Episode）的蒙特卡洛回报 Gt​ 来估计动作价值，并沿梯度方向 ∇θ​J(θ) 优化。
分类,无模型 (Model-free)、在线学习 (On-policy)、基于策略
区别,与价值基算法不同，它直接输出动作的概率分布，更适合处理连续动作空间。其回报估计方差大，收敛慢。
联系,是所有策略梯度方法的基础，是Actor-Critic方法的理论起点。

DQN (Deep Q-Network),Q-Learning 结合深度神经网络。
解决了高维输入（如图像）下的 Q-函数学习，成功应用于 Atari 游戏。

Double DQN / Dueling DQN,对 DQN 的改进。
解决 Q-值过高估计问题 (Double)，或分离状态价值和优势函数 (Dueling)，以提高学习效率。

2. 基于策略梯度 (Policy Gradient) 的 RL：这类算法直接优化策略函数 pi(s），通过梯度上升来找到能最大化奖励的策略。
RL 算法名称,核心机制,典型应用/特点
PPO (Proximal Policy Optimization),通过限制策略更新幅度来平衡探索与利用。
采样效率高，实现相对简单，是目前应用最广泛、最鲁棒的 RL 算法之一。在 VLA 和机器人学中常见。

3. Actor-Critic 与最大熵 RL (Maximum Entropy RL)：结合了值函数和策略函数的优势，并引入了最大熵原理来鼓励探索。

RL 算法名称,核心机制,典型应用/特点
DDPG (Deep Deterministic Policy Gradient),适用于连续动作空间的 Actor-Critic。
通过确定性策略简化了策略梯度计算，但对超参数敏感。

TD3 (Twin Delayed DDPG),对 DDPG 的改进。
引入双 Q 网络和延迟策略更新，以减少 Q-值过高估计和提升稳定性。

PPO和GRPO OPEN-VLA和OPEN-VLA-OFT   LoRA 微调/全量微调

SAC (Soft Actor-Critic),基于最大熵的 Actor-Critic 算法。
策略目标不仅是最大化奖励，还包括最大化熵（鼓励随机性），从而提升探索效率和稳定性。在连续控制任务中表现优异，常用于机器人控制。
