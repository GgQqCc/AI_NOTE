导纳和阻抗区别
力反馈求位姿   姿态反馈求力
拿鸡蛋不碎    推机器人/机械臂回到原位

flowmatching和 diffusion model区别



强化学习算法分类和区别
策略优化算法：
PPO   近端策略优化
GRPO  组策略优化
DPPO

post training
 DPPO 将整个扩散过程（策略 pi）视为一个MDP的内部步骤，利用PPO算法对整个策略进行优化
 利用RL的梯度，优化扩散策略参数，最大化环境回报。
1、EDM训练好的pi_theta 作为起点，训练一个独立的价值网络V_phi来估计状态的期望回报V_phi（s）
2、在环境中使用当前的策略pi_theta进行交互，收集一批数据（s_t, a_t, r_t, s_t+1）
s_t状态（V+L+位姿7维），a_t是扩散策略采样的动作序列 r_t是环境奖励
3、用收集的奖励计算累计回报R_t，用价值网络计算优势估计，A估计（Ahat_t）=用GAE方法，结合价值网络和环境奖励计算
4、（1）价值网络更新，V_phi（s_t）和R_t的MSE
（2）策略网络更新，Loss_theta = L_CLIP_theta+c1*L_VF_phi+c2*L_entropy_theta+L_KL_theta
L_CLIP_theta PPO裁剪项，通过概率比率来约束策略更新幅度，防止过远偏离。
Ratio_t_theta=pi_theta（a_t|s_t)/pi_theta_old(a_t|s_t)  解释：概率比率=theta的概率密度函数/theta_old的概率密度函数
L_VF_phi价值损失，L_entropy_theta熵损失，L_KL_theta KL散度约束
目标：最大化Loss_theta


奖励引导采样（RGS）：无需改变策略模型权重的post training，通过引入一个外部奖励函数梯度，推理阶段影响动作生成。
1、EDM训练好的pi_theta冻结
2、收集人/环境对不同完整动作序列的反馈（质量、偏好）
3、训练一个奖励模型R_psi(s，a）评估给定s情况下，动作序列a的预期奖励
4、推理阶段加入引导采样，标准动作从噪声a_k估计降噪后的动作a_k-1（或qieta_theta噪声)
引导采样：在采样的每几步，计算奖励模型R_psi(s，a）关于当前k时刻估计动作的梯度 nabla_a_k R_psi(s，a_k）
应用：把奖励梯度加到去噪步骤，引导模型向更高奖励方向生成动作。
公式：k-1时刻的引导动作a = k-1时刻标准动作a+lambda * scaling(k) * 奖励梯度
lambda引导强度，控制奖励对策略干预强度。lambda=0就是标准采样。
scaling(k) 衰减函数，一般扩散早起强引导，后期减弱。
完成扩散采样后，得到一个高奖励预期的动作序列。


Online RL 和 Offline RL：
在线:训练过程中实时地与环境进行交互,每当智能体执行一个动作并观察到新的状态和奖励时，
立即用这些新数据来更新它的策略Policy. 核心：主动采集、环境耦合边采边学，探索环境最优策略。
适用模拟环境/可大量试错环境 PPO、DQN、A2C
离线：batch RL，利用一个事先收集好，固定数据集(旧策略/人类专家数据)训练最优策略。
挑战：外推挑战（OOD问题outofdistribution）：1、有限数据训练，无新增，没法和在线rl一样实时交互要新数据。


RL和VLA结合



pi0和pi0.5
是层次化的VLA设计（双进程VLA),
RT-2 谷歌 PaLM-E 应用于机器人控制
OpenVLA （加了一层hybrid）
Diffuser-VLA
V-JEPA 自监督学习的最新方法（如JEPA），学习高效的视觉表征，目标是实现更高效、更通用的动作预测
DP-VLA
Gato：
双进程：低频推理（算力要求高） 高频控制。性能和效率平衡
L-Sys2、S-Sys1
高频1s两次，低频1s20次
低频：司机在任务的关键时刻或环境大变化时才进行深度思考：具体输出（潜变量z）：“现在需要上高速公路。”意图在几秒内固定不变，为高频执行设定任务目标
高频：就像司机的本能反应，是持续、实时的运动控制.
实时微调和校正
优劣： 长序列多步骤复杂任务用双进程VLA，复杂度高：需要分阶段训练或设计复杂的损失函数来协调高级和低级策略。



世界模型： 学习环境的动态和因果关系。 比如：推这个杯子 它会倒下。
1、感知模型：高维原始输入压缩成低纬抽象潜变量h_t
2、动态模型/状态转移模型：它预测潜变量在时间上的演变，预测下一时刻的抽象状态 ht+1，基于当前状态 h_t和智能体将要执行的动作 a_t。
3、重构/奖励模型：预测的抽象状态 h_t+1 重新映射回可解释的感官输出（如预测下一帧画面），并预测执行动作 a_t后能获得的奖励
功能：允许智能体在“想象”的世界中评估动作的优劣
Dreamer（核心原理）、Sora、World Labs
