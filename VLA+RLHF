标准VIT和 hybrid（混合） VIT的区别：混合策略：减少计算提取局部特征
标准是把图像分割成不重叠的图像patch，线性层把每个块映射为token。
混合是用一个CNN 残差网络（ResNet）作为tokenizer（分词器），取代简单分块和线性投影。

1、CNN残差网络——1D卷积——激活函数（Relu）——池化
目的：提取图像的低级到中级特征，同时压缩图像的空间维度
VLA：视觉: hybrid VIT 架构，输入：多时步 双输入RGB数据
1D卷积，应用于时间序列，沿着patch Token序列。捕捉相邻时间块关系。
池化：patching前，压缩空间分辨率，降低后续计算负担，保留局部纹理信息。
时间切块，4个图像patch特征压缩成一个低纬token（来自CLIP)
采集网格（224*224）+1000个随机点。 随机点选方差大的（变动大的）
2、文本SBERT
3、位姿 归一化/反归一化用于将机器人状态或动作从原始的物理范围归一化到 [-1, 1]

多模态融合（视觉+文本+位姿）
Perceptual Transformer 原生多模态技术（基于Perceiver，扩展来适应时间序列），对齐所有感知输入，生成统一时序特征序列，为策略网络提供state和action表示。
Perceiver:

固定大小的很小的潜在阵列处理所有输入，解耦计算复杂度和输入大小。
交叉注意力机制统一处理和融合不同模态数据。
时间序列视为可以被压缩和统一处理的高维字节流。

通过力减少误差累积

Diffusion-based VLA 框架中，生成策略 pi由Transformer 模型来实现，扮演传统图像
DiffusionModel U-Net 的角色（DiT）
DiT工作原理：保留了 U-Net 多尺度处理思想，TransformerBlock替换传统的卷积层，实现全局交互和多模态融合。
策略pi的目标是根据当前时间步t、视觉I和语言指令L，预测或估计动作序列A的噪声epsilon，逐步“去噪”并生成最终的动作序列
DiT核心结构：Transformer Block 的 U-Net 布局：不是标准U-Net卷积层堆叠，是Transformer模块层次化布局
下采样路径Encoder:输入序列S(所有输入统一token 序列S:)传入一系列 Transformer Block，
减少 token 序列的长度（类似于U-Net中的池化/下采样），捕捉动作、视觉和语言的宏观、高级特征。
瓶颈层 (Bottleneck):最低分辨率处，模型通过一/多个Transformer Block捕捉全局语义（任务目标、环境概况）
上采样路径 (Decoder):Transformer Block 恢复 token 序列的长度。
执行跳跃连接 (Skip Connections)，将下采样路径中对应层次的特征（token）融合进来（通过拼接或加法）。
确保了在生成动作时，能同时利用宏观的语义信息（瓶颈层）和微观的局部细节（跳跃连接）。
生成的核心：去噪预测和注意力,去噪预测：多层transformer处理，输出集中在对动作token预测，预测值是该时间步t所需的噪声估计epsilon_theta
DiT训练完成后，动作序列A生成过程是迭代去噪，
总结：Transformer的全局注意力来代替U-Net的卷积，实现了高效的多模态融合，并将整个动作生成过程转化为一个迭代的去噪过程

采样（去噪）数学原理：1、逆向马尔科夫链，目标：最小化噪声预测误差
数学原理：逆向 常微分方程ode 和随机微分方程sde
加噪是：sde 加随机噪声，逆向可以sde也可以ode。 我们逆向是ode。
残差网络(Skip Connections)：
梯度消失/爆炸： 随着网络层数的增加，反向传播过程中梯度要么变得极小（消失），要么变得极大（爆炸），导致模型难以训练。
一个残差块的核心思想是让网络层学习残差函数H(x) - x，而不是直接学习输出 $H(x)$。
通过残差块的设计，有效地解决了深度神经网络的性能退化问题
U-net 一种cnn架构

扩散模型策略Edm：
原理：1、Karras噪声调度（先快后慢步进路线）
2、引入微小晃动（随机性Churn），模型先去噪，再加一点噪声再去噪。打破确定性路径僵局。
使用原因：1、鲁棒性好，解决多模态分布问题 2、支持高维输出 3、支持多个有效序列（动作多，适合复杂项目场景）
一次性规划未来10步的action trunk

优化器：AdamW【自适应学习率（靠动量和步长），梯度变化大步长小，变化小步长大；收敛快；W：移除权重衰减】
稳定器EMA(最小化预测噪声和实际噪声差异)，学习率三阶段学习率调度器，采样器：海恩法（起点斜率和终点斜率平均值，常微分方程求解）。
损失函数：动作+beta*策略
动作损失：得分匹配损失，预测噪声和实际添加噪声之间MSE
策略损失：生成未来轨迹和真实轨迹之间的L2损失。

transformer：自注意力机制，动态给每个位置分配不同权重，捕捉任意两个位置依赖关系。
通过Q、 K 、V之间点积得到每个词和其他词相关性权重  查询矩阵、键矩阵、值矩阵。
多头注意力：输入分成多个子空间 独立计算注意力，拼接结果。 关注模型不同部分如语法、语义。 并行计算

RLHF：（优化或微调）SAC
奖励模型： r
1、任务成功的稀疏奖励
2、夹爪和目标区域欧式距离，鼓励快速接近的效率。密集奖励
3、力矩惩罚（通过力控检测）
4、人类标注其他异常行为惩罚。

A：（策略网络 pi therta）：决定在给定状态 s下采取哪个动作。
C：（价值网络 V phi）：评估当前状态 s的价值，学习状态价值函数V(s)。
PPO：无模型 在线RL算法，基于AC框架，解决核心：策略梯度方法中，较大的策略更新步长可能导致训练不稳定甚至崩溃。
引入裁剪机制，限制新策略 pi{theta{new}} 相对于旧策略 pi{theta{old}}$ 的变化幅度，从而实现了稳定、快速的策略更新。
损失函数：策略+价值+熵损失
流程：
actor与环境交互 收集一批数据：s a s'
计算每个step 优势估计、目标价值
优化：AC网络多次小批量梯度上升下降
新策略更新

轨迹T放入奖励模型r中，每个轨迹评估输出奖励值。
使用 PPO 来策略对齐，更新VLA策略pi参数，最大化奖励模型的累计奖励。
PPO用裁剪代理损失保证更新稳定性和效率，加入了KL散度约束防止灾难性遗忘。

超参数： 学习率，Transformer层数 裁剪参数 GAE平滑因子  epoches
仿真：Isaac  摄像头：realsense

难点：
数据稀疏性，现实成本数据成本高，simtoreal gap大
实时性要求高，规划不能延迟。解决：高效的感知-控制协同结构设计
软硬件兼容，接口协议复杂。
鲁棒和可靠性  光照、零件公差、机械臂等硬件误差、干扰
理论困难：

调试 重构 优化 部署 质量 优势

基于流/ 扩散模型的VLA
VLA 模型名称,核心机制,特点
Diffusion Policy,使用 扩散模型 (Diffusion Model) 来建模动作序列的条件分布。
能够生成高质量、多模式的动作，在处理多任务和不确定性高的环境时表现优异。
πRL​ (如上文),基于流的 VLA，将 RL 融入训练，提升在线适应性。
专注于解决 VLA 模型的在线微调和动作对数似然计算问题。

强化学习：
1. 基于值函数(Value-Based) ：主要学习一个值函数，用来估计在特定状态或状态-动作对下能获得的预期累积奖励
Q-Learning,"学习最优的 Q-函数 Q(s,a)。" DQN
适用于离散状态和动作空间，是 Model-Free RL 的基础。
2、基于策略（Policy-Based）：核心是直接学习一个策略函数pi(a|s)，并尝试最大化其带来的期望回报。
算法, REINFORCE：原理：使用策略梯度直接更新策略参数 threata。它通过一个完整回合（Episode）的蒙特卡洛回报Gt来估计动作价值，并沿梯度方向优化。
3、AC算法：PPO 在Actor更新中引入裁剪惩罚，确保新旧策略差距小。

原理,使用策略梯度定理直接更新策略参数 θ。它通过一个完整回合（Episode）的蒙特卡洛回报 Gt​ 来估计动作价值，并沿梯度方向 ∇θ​J(θ) 优化。
分类,无模型 (Model-free)、在线学习 (On-policy)、基于策略
区别,与价值基算法不同，它直接输出动作的概率分布，更适合处理连续动作空间。其回报估计方差大，收敛慢。
联系,是所有策略梯度方法的基础，是Actor-Critic方法的理论起点。

DQN (Deep Q-Network),Q-Learning 结合深度神经网络。
解决了高维输入（如图像）下的 Q-函数学习，成功应用于 Atari 游戏。

Double DQN / Dueling DQN,对 DQN 的改进。
解决 Q-值过高估计问题 (Double)，或分离状态价值和优势函数 (Dueling)，以提高学习效率。

2. 基于策略梯度 (Policy Gradient) 的 RL：这类算法直接优化策略函数 pi(s），通过梯度上升来找到能最大化奖励的策略。
RL 算法名称,核心机制,典型应用/特点
PPO (Proximal Policy Optimization),通过限制策略更新幅度来平衡探索与利用。
采样效率高，实现相对简单，是目前应用最广泛、最鲁棒的 RL 算法之一。在 VLA 和机器人学中常见。

3. Actor-Critic 与最大熵 RL (Maximum Entropy RL)：结合了值函数和策略函数的优势，并引入了最大熵原理来鼓励探索。

RL 算法名称,核心机制,典型应用/特点
DDPG (Deep Deterministic Policy Gradient),适用于连续动作空间的 Actor-Critic。
通过确定性策略简化了策略梯度计算，但对超参数敏感。

TD3 (Twin Delayed DDPG),对 DDPG 的改进。
引入双 Q 网络和延迟策略更新，以减少 Q-值过高估计和提升稳定性。

SAC (Soft Actor-Critic),基于最大熵的 Actor-Critic 算法。
策略目标不仅是最大化奖励，还包括最大化熵（鼓励随机性），从而提升探索效率和稳定性。在连续控制任务中表现优异，常用于机器人控制。
